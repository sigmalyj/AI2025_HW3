**题目复述**
本题研究GPT类架构在机器翻译任务中自注意力机制的计算过程。给定中文句子“他|喜欢|苹果”（3个token），在GPT的某一自注意力层中，每个token的query、key、value向量分别记为：
• $ Q = \{q_1, q_2, q_3\} $

• $ K = \{k_1, k_2, k_3\} $

• $ V = \{v_1, v_2, v_3\} $

其中，$ q_i, k_i, v_i \in \mathbb{R}^d $。

**问题1**  
写出经过带掩码的自注意力层（公式中省略掩码）后，每个token对应的输出 $ y_i $ 的表达式：  
$$
Y = \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$

**问题2**  
假设向量维度 $ d = 4 $，且已知Key矩阵：  
$$
K = \begin{pmatrix}
0 & 1 & 0 \\
1 & 0 & -1 \\
-1 & -1 & 0 \\
0 & 1 & -1
\end{pmatrix}
$$  
当模型预测翻译的第一个token（英文单词“He”）时，需要让模型重点关注第一个token“他”的信息。  
要求：  
1. 给出 $ q_3 $ 的一个取值，满足：
   • $ \|q_3\|_2 \leq 1 $（2范数不超过1）；

   • 在计算 $ y_3 $ 时，$ q_3 $ 与第一个token $ k_1 $ 的自注意力权重最大（相比其他token）。  

2. 写出此时 $ y_3 $ 关于 $ v_1, v_2, v_3 $ 的表达式。

---

**作答**

**问题1**  
自注意力层的输出 $ y_i $ 的表达式为：  
$$
y_i = \sum_{j=1}^3 \frac{\exp\left(\frac{q_i k_j^T}{\sqrt{d}}\right)}{\sum_{l=1}^3 \exp\left(\frac{q_i k_l^T}{\sqrt{d}}\right)} v_j, \quad i = 1, 2, 3
$$  
其中，Softmax对每行归一化，权重表示当前token $ q_i $ 对其他token $ k_j $ 的关注程度。

---

**问题2**  
（1）$ q_3 $ 的取值  
为使 $ q_3 $ 与 $ k_1 $ 的注意力权重最大，需满足：  
$$
q_3 k_1^T > q_3 k_2^T \quad \text{且} \quad q_3 k_1^T > q_3 k_3^T
$$  
已知 $ k_1 = [0, 1, -1, 0]^T $，$ k_2 = [1, 0, -1, 1]^T $，$ k_3 = [0, -1, 0, -1]^T $。  

选择 $ q_3 = [1, 0, 0, 0] $（满足 $ \|q_3\|_2 = 1 $）：  
• $ q_3 k_1^T = 1 \times 0 + 0 \times 1 + 0 \times (-1) + 0 \times 0 = 0 $  

• $ q_3 k_2^T = 1 \times 1 + 0 \times 0 + 0 \times (-1) + 0 \times 1 = 1 $  

• $ q_3 k_3^T = 1 \times 0 + 0 \times (-1) + 0 \times 0 + 0 \times (-1) = 0 $  


此时 $ q_3 k_2^T $ 最大，但题目要求 $ q_3 $ 与 $ k_1 $ 的权重最大，因此需调整。  
修正方案：  
选择 $ q_3 = [0, 1, 0, 0] $：  
• $ q_3 k_1^T = 1 $（最大），$ q_3 k_2^T = 0 $，$ q_3 k_3^T = -1 $。  

满足 $ q_3 k_1^T > q_3 k_2^T $ 且 $ q_3 k_1^T > q_3 k_3^T $，且 $ \|q_3\|_2 = 1 $。  

最终取值：  
$$
q_3 = [0, 1, 0, 0]
$$

（2）$ y_3 $ 的表达式  
计算注意力权重（$ d = 4 $，$ \sqrt{d} = 2 $）：  
$$
\text{Softmax输入} = \frac{q_3 K^T}{2} = \frac{[0, 1, 0, 0] K}{2} = \frac{[1, 0, -1]}{2}
$$  
Softmax归一化：  
$$
\text{权重} = \left[\frac{e^{0.5}}{e^{0.5} + e^0 + e^{-0.5}}, \frac{e^0}{e^{0.5} + e^0 + e^{-0.5}}, \frac{e^{-0.5}}{e^{0.5} + e^0 + e^{-0.5}}\right]
$$  
因此：  
$$
y_3 = \frac{e^{0.5}}{S} v_1 + \frac{e^0}{S} v_2 + \frac{e^{-0.5}}{S} v_3, \quad S = e^{0.5} + e^0 + e^{-0.5}
$$  
化简后：  
$$
y_3 = \frac{e^{0.5}}{e^{0.5} + 1 + e^{-0.5}} v_1 + \frac{1}{e^{0.5} + 1 + e^{-0.5}} v_2 + \frac{e^{-0.5}}{e^{0.5} + 1 + e^{-0.5}} v_3
$$  

---

**关键点总结**
1. 问题1：自注意力输出是value向量的加权和，权重由Query-Key点积经Softmax决定。  
2. 问题2：  
   • 通过设计 $ q_3 $ 与 $ k_1 $ 的点积最大化，实现模型对“他”的聚焦。  

   • 注意向量的2范数约束和Softmax的分母（归一化因子）。
