# 人工智能导论第3次作业

<center>
    姓名：李炎佳 	学号：2022030078	班级：软33
</center>



## 简答题

1.

**交叉熵定义**：

给定两个概率分布 $ p $ 和 $ q $，$ p $ 相对于 $ q $ 的交叉熵定义为：

$$
H(p, q) = E_p[-\log q] = H(p) + D_{KL}(p \| q)
$$

其中：
- $ H(p) $ 是 $ p $ 的熵，
- $ D_{KL}(p \| q) $ 是从 $ p $ 到 $ q $ 的 KL 散度（也称为 $ p $ 相对于 $ q $ 的相对熵）。

**离散分布形式**

对于离散分布 $ p $ 和 $ q $，交叉熵表示为：

$$
H(p, q) = -\sum_x p(x) \log q(x)
$$

**连续分布形式**

对于连续分布，假设 $ p $ 和 $ q $ 在测度 $ r $ 上是绝对连续的（通常 $ r $ 是 Borel σ-代数上的 Lebesgue 测度）。设 $ P $ 和 $ Q $ 分别为 $ p $ 和 $ q $ 的概率密度函数，则交叉熵为：

$$
-\int_X P(x) \log Q(x) \, dr(x) = E_p[-\log Q]
$$

- $ E_p[\cdot] $: 基于分布 $ p $ 的期望。
- $ \log $: 自然对数（以 $ e $ 为底）。
- $ D_{KL}(p \| q) $: KL 散度，衡量分布 $ p $ 和 $ q $ 的差异。

**绝对值损失定义**：
$$
L_{\text{abs}} = \sum_{i} |y_i - \hat{y}_i|
$$

**交叉熵的优势**：

- 交叉熵的梯度与误差成正比（$\nabla_{\hat{y}} L \propto (y - \hat{y})$），梯度更新更直接，适合梯度下降优化。而绝对值损失的梯度是常数（$\pm 1$），无法根据误差大小动态调整，优化效率低。
- 交叉熵是凸函数，保证梯度下降能找到全局最优解，而绝对值损失在分类问题中可能导致非凸优化问题。

2.

**MLP 相比线性模型的优势**：

- 非线性建模：MLP可以通过ReLu等激活函数引入非线性变换，使其能够逼近任意连续函数，具有更强大的建模能力
- 更强的表达能力：MLP通过增加隐藏层和神经元数量，可以灵活调整模型容量，适用于高维数据和大规模任务。而线性模型的表达能力受限于输入特征的线性组合，难以处理高复杂度问题。

**窄而深的神经网络的好处**：

- 参数利用更加高效：窄而深的网络通过逐层抽象，能够以更少的参数学到更复杂的特征表示。相比之下，浅而宽的网络需要大量神经元来拟合相同复杂度的函数，导致参数冗余和计算浪费。
- 层次化特征学习：深度网络通过多个非线性层逐步提取低层到高层的特征，符合自然数据的层次化结构。而浅层网络难以有效建模这种层次化特征，可能影响模型性能。
- 泛化能力更好：深度网络通过正则化技术（如Dropout、BatchNorm）和残差连接（ResNet）缓解过拟合，而浅层宽网络因参数量大更容易过拟合，尤其在数据量不足时表现更差。

3.

 对两个函数 $f$ 和 $g$

连续和离散卷积运算定义为：  
$$
(f*g)(t)=\int_{-\infty}^\infty f(\tau)g(t-\tau)\mathrm{d}\tau\\
(f * g)(n) = \sum_{m=-\infty}^{\infty} f(m) \cdot g(n - m)
$$
连续和离散互相关运算定义为：  
$$
\begin{gathered}
(f\star g)(n)=\sum_{m=-\infty}^\infty\overline{f(m)}g(m+n) \\
[f(t)\star g(t)](t)=[\overline{f(-t)}*g(t)](t)
\end{gathered}
$$
在CNN中，卷积层实际执行的是互相关（Cross-correlation）操作，而非严格数学定义的卷积。
这是因为CNN的卷积核本身就是通过数据学习得到的，学习过程中核会自动适应方向，翻转操作是冗余的。且互相关直接计算输入与核的局部相似性，物理意义更明确。


4.



5.

**残差连接如何帮助训练更深层的网络？**

深度神经网络在层数增加时，往往会遇到两个核心问题：梯度消失和网络退化。残差连接（Residual Connection）的提出，正是为了解决这些问题，使得训练超深层网络成为可能。  

1. **梯度消失问题的缓解**  
在传统深层网络中，反向传播的梯度需要通过链式法则逐层传递。如果每一层的梯度值较小（例如由于激活函数饱和或权重初始化不当），深层网络的梯度会呈现指数级衰减，导致底层的参数几乎无法更新。这种现象称为梯度消失。  

残差连接通过引入跨层的直连路径，改变了梯度的传播方式。具体来说，残差块的输出可以表示为：  
$$
y = F(x) + x
$$  
其中，$ F(x) $ 是神经网络需要学习的残差函数，而 $ x $ 是输入的直接映射。在反向传播时，梯度计算变为：  
$$
\frac{\partial y}{\partial x} = \frac{\partial F(x)}{\partial x} + 1
$$  
即使 $ \frac{\partial F(x)}{\partial x} $ 接近于 0（即梯度消失），由于存在 $ +1 $ 项，梯度仍然能够有效回传到底层。这一机制相当于在反向传播时提供了一条低阻力的梯度通路，使得深层网络的训练更加稳定。  

2. **网络退化问题的解决**  
实验观察发现，单纯增加网络深度时，测试误差有时反而会上升，这种现象并非由于过拟合，而是因为深层网络难以学习到比浅层网络更优的映射。这种现象被称为网络退化（Degradation）。  

残差连接的巧妙之处在于，它将网络的优化目标从直接学习 $ H(x) $（即输入到输出的完整映射）转变为学习残差函数 $ F(x) = H(x) - x $。如果最优的映射接近于恒等映射（即 $ H(x) \approx x $），那么网络只需要让 $ F(x) $ 趋近于 0，这比直接拟合 $ H(x) = x $ 更容易优化。因此，残差结构使得深层网络能够更高效地训练，即使层数大幅增加，性能也不会显著下降。


**残差连接能否缓解梯度消失？**  
是的，残差连接能够显著缓解梯度消失问题，原因可以总结为以下几点：  

1. 梯度直通机制：残差路径（即 $ +x $ 部分）提供了一条不受非线性变换影响的梯度传播路径，确保梯度能够直接回传到底层，避免因多层累积导致的梯度衰减。  

2. 梯度幅度保持：在传统网络中，如果某一层的梯度 $ \frac{\partial F(x)}{\partial x} $ 接近于 -1，梯度会迅速衰减至 0；而在残差网络中，由于存在 $ +1 $ 项，梯度仍然能够保持合理的幅度（例如 $ -0.9 + 1 = 0.1 $），使得反向传播更加稳定。  

3. 实际训练效果：ResNet 的成功应用（如训练超过 1000 层的 ResNet-1202）表明，残差连接能够有效支持超深层网络的训练，而传统网络在类似深度下几乎无法收敛。  




## 解答题

2.1







2.2



感知机的收敛保证





## 深度学习与AlphaZero

1.



2.



3.



4.







## **参考资料和辅助工具**

### **参考资料**

1. 课件
2. https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5
3. 

### 辅助工具

使用DeepSeek R1询问知识点学习、辅助实验数据可视化及理解项目框架

